 
CS237 Spring 2016 ­ Distributed Systems Middleware 
 

 
Under guidance of Prof. Nalini Venkatasubramanian 
 
 
 
Blaze ­ IoT Analytics Engine and performance 
comparison with relational engine 
 
 
 
 
 
Prashanth Reddy Billa (60804572) 
Sri Ranga Teja Kolli (31152056) 
Sudeep Meduri (51884271) 
  

 
Goal 

To   develop   a   prototype   of   an   end   to   end   system   where   various   sensor’s   data   is   continuously   ingested  
into   the   Hadoop   distributed   file   system   and   then,   based   on   this   data   user   can   perform   analytics  
through   hive   queries.   The   system   has   two   modes   ­   basic   and   advanced.   Also   performance   comparison  
is done with the traditional MySQL system. 
 
*************************************************************************** 
 
Motivation 

Internet of Things or just IoT provides us with a great level of awareness about our world and our ecosystem. It is a platform from which we can monitor the reactions to the changing conditions that said awareness exposes us to. According to Bob Metcalfe, the value of a communications network is proportional to the square of the number of connected compatible communicating devices and this is just fascinating. Now in case we connect all the sensing devices together in such a way as to manage them as a larger system, we can exploit the Metcalfe effect and then reap out the value from a wide variety of analytics. Analytics could be done for pure business purposes, or could also be done from the society perspective as in disaster alert system and management, smart cities etc.
 
*************************************************************************** 
 
Introduction 

We realize that Big Data is mainly contributed by the IOT cloud and is mostly generated by sensor nodes and devices at the edges of networks. Such data is required to be processed like data in the clouds which can be public, private or enterprise cloud. Companies and individuals in today’s world constantly acquire vast volumes of data having high velocity and veracity from different sources and leverage this information by means of data science and statistical analytical techniques to support effective decision­making and provide new functionality, products and services. The infrastructure which we would be using should be distributed. Few other key requirements are the availability by providing failover clustering. Lastly the scalability plays a critical role in deciding the stability of the architecture in today’s big data world.
 
Data analytics must be performed on immense volume of data and the relevant data must be extracted, processed, queried and analyzed in real time in case of OLTP and in a good amount of time in case of OLAP. The MapReduce framework is used for processing as it provides a good level of fault tolerance through replication in the distributed file system, scalability and improved performance by parallel computing and also good level of flexibility. Traditional MapReduce has been little arduous, but now with tools such as Impala, Hive and Pig, it is very convenient to get the desired information and analytics out of the big data. MapReduce has become the most popular framework for large­scale processing and analysis of huge data sets in clusters of machines, mainly because of its performance and access to various tools to make the task easier. Hadoop implementation of MapReduce has contributed to its widespread usage both in industry and academia.

We developed a generic architecture which could be used by any IoT based project to log details which could be used for different analytics.Within the scope of the project, we implemented an end to end system. The modules could be mentioned as 1) IoT prototype applications 2) MQTT broker 3) Listener services 4) Hadoop ecosystem We worked on implementing three prototype applications, two on which we would run using different analytics. We have a developed a service which emulates the front end based on synthetic data. We are utilizing the SCALE( Safe Community Alert Network ) project’s MQTT broker. We have listener services (subscribers) which listen to different events’ data and push it into hdfs.

*************************************************************************** 

Work on IoT prototype applications 
 

Our initial scope of the project included three prototype systems. One a test application ­ a phone app which continuously publishes the user’s geolocation coordinates. A second application which would showcase management of power consumption in IoT devices. Tried to implement it as a part of the existing “Fall detection system”. The idea is to cut off power supply to the pressure sensor network when the person isn't home. There would be two updates to the code ● Adding a flag bit in the python code ­ Rapsberry Pi. This flag is to be set based on signal communication from the existing phone app. Could use your help with this. We would send this flag bit whenever the person is 300 feet as an example from the defined geofence. The center of fence would be “location” : {“lat” : “39.08301”, “lon” : “​ 77.149629”}, ● Including a portion in the phone code which checks if person is within the fence or not. If within, send 1 to pi, else send 0 to pi. On further work, realized that controlling the power supply to the raspberry pi wouldn’t be possible without additional hardware ­ for example, a “sleepy pi” board And the third application was to address the problem of defining person’s presence within a building with a linear topology Wi­Fi infrastructure. However, further survey did help us to realize building a new system altogether isn’t needed, as existing systems could be utilized by deploying “beacons” in the area which would now act as additional access points, eliminating the problem.
 
.​
*************************************************************************  
System Components 

● Data Ingestion ○ IoT application emulators Based on real time data from existing systems ­ we determined ranges and deployed emulation systems for seismic, air pollution, humidity, traffic, which publish through different channels in MQTT ○ Real time IoT application events data We are utilizing the SCALE( Safe Community Alert Network ) project applications generated real­time data. ● Processing Engine ○ The core processing engine is the Hadoop MapReduce and Hive which are based upon the data in Hadoop Distributed File System. 5 ● Web Interface ○ Login and Registration ○ Basic Mode ■ In this mode, we have 6 different sensor types: ● Air Pollution ● Seismic ● Traffic ● Humidity ● Fall Detection ● Cross Correlation (Hybrid / Mix of two or more sensors) ■ Once the sensor is selected, dynamically sensor specific actions (queryable items) get populated. The user can then select any one action and press the Go button which triggers the execution of query on our core processing engine and generated text/visual results. ○ Advanced Mode ■ In this mode, the user can upload his own sensor data file and then perform HiveQL queries on the data. This is also a very good interface to perform some ad hoc testing and analysis. ○ Executed Tasks By user ■ This section displays the tasks performed by the user, sorted by date where latest queries and their results are shown first. Also the table has an AJAX search implemented and thus, an user can search for a specific information by just typing in the desired word/sentence/characters and pressing enter. Also the result of the query performed is available in this section. ○ Azure Hadoop Environment Status ■ This is to let the user know if the remote Azure Hadoop environment is up and running. ○ Results Page ■ This page is where the user is redirected to once he performs the HiveQL queries in advanced mode/ selects a specific sensor action in basic mode. We display the text/visual results to the user. ○ JSON sensor viewer for basic mode queries ■ The user can see the sensor JSON structure with a tree view. This is for the user to better understand the JSON sensor data format. This is part of the results page when queries are performed through basic mode.
 
***************************************************************************  

Tools and Environment 
 
Core Processing Engine 
● Azure cloud service:  


We   have   setup   the   complete   processing   environment   on   the   Microsoft’s   Azure  
platform.   All   our   queries   are   processed   by   the   Hadoop   in   Azure.   Also   in   advanced  
mode,   we   perform   an   SFTP   transfer   of   user’s   custom   JSON   file   into   the   Azure   cloud  
and apply his custom query and then display the results in the user interface. 
Additionally,   we   also   have   setup   the   complete   hadoop   and   hive   environment   on   our  
linux machine for ad hoc testing. 
 
● Hadoop Version 2​
○ Hadoop Distributed File System:  
○ Yarn Cluster Resource Management 
○ MapReduce V2 
● HiveServer2: 
○ Hive Thrift Server 
○ Hive Driver 
○ MetaStore 
○ Apache Derby RDBMS 
● Hive JDBC driver:  

Data Ingestion System 
● Python 

The python scripts to generate the sensor data and publish them to a specific channel of our MQTT broker reside on our local machines. The python scripts which would be the data ingestion to our system reside on the remote Azure platform and they are continuously run and the sensor JSON files are updated constantly. Whenever a user performs a query in basic mode, the latest snapshot of the sensor data file is captured and the query is then performed on it, and the results are displayed in the user interface. The ingestion for our engine with four different publishing data to the mqtt broker was observed to be 3680 json objects per minute.
 
MQTT Broker 

We have chosen to go with the MQTT broker approach over the HTTP REST interface due to the following three factors. ○ Less battery consumption for the sensor and end devices. ○ Less bandwidth usage ○ Low latency compared to the REST alternative MQTT acts as a pub­sub system, where a publisher publishes to a topic and clients can subscribe to any number of topics.

Web User Interface ● Java Server Pages JSP is used to perform server side programming, query the MySQL database for user’s information, maintain sessions etc. ● HTML5/CSS3/Javascript Used for the user interface, styling and user side validation, displaying graphs, dynamic manipulation of HTML Document Object Model elements etc. ● MySQL The traditional MySQL relational database is used to maintain the user’s profile information and is used for login, registration and storing user’s query history for displaying in dashboard. ● MySQL Connector for Java MySQL JDBC driver is used as an interface between the Java programs (JSP) and the MySQL database.
*************************************************************************** 
 
Comparison with MySQL system 
 
Running Time vs Dataset Size for our Hadoop engine and MySQL engine 
 
Dataset size  MySQL engine (seconds)  Hadoop Engine (seconds) 
2 Lakh  1.7  19.06 
2 Million  7.09  28.9 
6 Million  18.78  36.76 
10 Million  37.73  60.7 
50 Million  193.14  195.18 
70 Million  261  250.00 
90 Million  355.21  328.67 
 
 

 
In addition to the improvement in response time for very large data set, Hadoop has a number of other reasons to be chosen as the primary choice for real time and big data analytics. ● There is no form a structured model of the data and create the relations required beforehand as in relational databases. This greatly reduces the overhead in Extract Transform and Load which is to be performed in case of relational databases such as Oracle and MySQL. Thus, a vast amount of raw data in semi structured and unstructured formats can be fed into the HDFS and then scan, analyze and queries could be performed. ● As an open­source software framework, Hadoop runs on standard servers. Hardware can be added or swapped in or out of a cluster, and operational costs are relatively low because the software is common across the infrastructure, requiring little tuning for each physical server. ● The partitioning and sharding logic can be explicitly mentioned and thus, this can lead to data being sequentially accessed often put into the same data nodes, unlike the relational MySQL clusters where users do not have control over the auto­sharding strategy employed.

Conclusions and Future Scope 

Having developed a prototype for an end to end system from scratch, the possibilities are endless. Mainly, the querying could be performed using complex machine learning algorithms and also scripting approaches to perform MapReduce such as Pig Latin usage could be integrated into the system. ● Alerting/notification feature is one of the most important and helpful feature to have. This could be well integrated into our system. ● Also sensor data files in formats other than JSON such as XML etc could be supported. ● Apache Spark and its beauty of in memory computations and all of its advantages could be leveraged.


 
 
